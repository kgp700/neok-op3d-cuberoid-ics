#include <linux/kernel.h>#include <linux/module.h>#include <linux/init.h>#include <linux/notifier.h>#include <linux/cpufreq.h>#include <linux/delay.h>#include <linux/interrupt.h>#include <linux/spinlock.h>#include <linux/device.h>#include <linux/slab.h>#include <linux/cpu.h>#include <linux/completion.h>#include <linux/mutex.h>#include <linux/timer.h>#define dprintk(msg...) cpufreq_debug_printk(CPUFREQ_DEBUG_CORE, \						"cpufreq-core", msg)static struct cpufreq_driver *cpufreq_driver;static DEFINE_PER_CPU(struct cpufreq_policy *, cpufreq_cpu_data);#ifdef CONFIG_HOTPLUG_CPUstatic DEFINE_PER_CPU(char[CPUFREQ_NAME_LEN], cpufreq_cpu_governor);#endifstatic DEFINE_SPINLOCK(cpufreq_driver_lock);#ifdef CONFIG_HAS_EARLYSUSPENDstatic void cpufreq_early_suspend( struct early_suspend *h );static void cpufreq_early_resume( struct early_suspend *h );#endif static DEFINE_PER_CPU(int, cpufreq_policy_cpu);static DEFINE_PER_CPU(struct rw_semaphore, cpu_policy_rwsem);#define lock_policy_rwsem(mode, cpu)					\int lock_policy_rwsem_##mode						\(int cpu)								\{									\	int policy_cpu = per_cpu(cpufreq_policy_cpu, cpu);		\	BUG_ON(policy_cpu == -1);					\	down_##mode(&per_cpu(cpu_policy_rwsem, policy_cpu));		\	if (unlikely(!cpu_online(cpu))) {				\		up_##mode(&per_cpu(cpu_policy_rwsem, policy_cpu));	\		return -1;						\	}								\									\	return 0;							\}#define HIGHFREQ 1#define LOWFREQ 2#define TRUE 1#define FALSE 0unsigned int max_policy = FALSE; EXPORT_SYMBOL_GPL(max_policy); lock_policy_rwsem(read, cpu);EXPORT_SYMBOL_GPL(lock_policy_rwsem_read);lock_policy_rwsem(write, cpu);EXPORT_SYMBOL_GPL(lock_policy_rwsem_write);void unlock_policy_rwsem_read(int cpu){	int policy_cpu = per_cpu(cpufreq_policy_cpu, cpu);	BUG_ON(policy_cpu == -1);	up_read(&per_cpu(cpu_policy_rwsem, policy_cpu));}EXPORT_SYMBOL_GPL(unlock_policy_rwsem_read);void unlock_policy_rwsem_write(int cpu){	int policy_cpu = per_cpu(cpufreq_policy_cpu, cpu);	BUG_ON(policy_cpu == -1);	up_write(&per_cpu(cpu_policy_rwsem, policy_cpu));}EXPORT_SYMBOL_GPL(unlock_policy_rwsem_write);static int __cpufreq_governor(struct cpufreq_policy *policy,		unsigned int event);static unsigned int __cpufreq_get(unsigned int cpu);static void handle_update(struct work_struct *work);static work_func_t handle_rollback(void *data);static DECLARE_DELAYED_WORK(cpufreq_tom_rollbackwq, (work_func_t) handle_rollback);static BLOCKING_NOTIFIER_HEAD(cpufreq_policy_notifier_list);static struct srcu_notifier_head cpufreq_transition_notifier_list;static bool init_cpufreq_transition_notifier_list_called;static int __init init_cpufreq_transition_notifier_list(void){	srcu_init_notifier_head(&cpufreq_transition_notifier_list);	init_cpufreq_transition_notifier_list_called = true;	return 0;}pure_initcall(init_cpufreq_transition_notifier_list);static LIST_HEAD(cpufreq_governor_list);static DEFINE_MUTEX(cpufreq_governor_mutex);struct cpufreq_policy *cpufreq_cpu_get(unsigned int cpu){	struct cpufreq_policy *data;	unsigned long flags;	if (cpu >= nr_cpu_ids)		goto err_out;	spin_lock_irqsave(&cpufreq_driver_lock, flags);	if (!cpufreq_driver)		goto err_out_unlock;	if (!try_module_get(cpufreq_driver->owner))		goto err_out_unlock;	data = per_cpu(cpufreq_cpu_data, cpu);	if (!data)		goto err_out_put_module;	if (!kobject_get(&data->kobj))		goto err_out_put_module;	spin_unlock_irqrestore(&cpufreq_driver_lock, flags);	return data;err_out_put_module:	module_put(cpufreq_driver->owner);err_out_unlock:	spin_unlock_irqrestore(&cpufreq_driver_lock, flags);err_out:	return NULL;}EXPORT_SYMBOL_GPL(cpufreq_cpu_get);void cpufreq_cpu_put(struct cpufreq_policy *data){	kobject_put(&data->kobj);	module_put(cpufreq_driver->owner);}EXPORT_SYMBOL_GPL(cpufreq_cpu_put);#ifdef CONFIG_CPU_FREQ_DEBUGstatic unsigned int debug;static unsigned int debug_ratelimit = 1;static unsigned int disable_ratelimit = 1;static DEFINE_SPINLOCK(disable_ratelimit_lock);static void cpufreq_debug_enable_ratelimit(void){	unsigned long flags;	spin_lock_irqsave(&disable_ratelimit_lock, flags);	if (disable_ratelimit)		disable_ratelimit--;	spin_unlock_irqrestore(&disable_ratelimit_lock, flags);}static void cpufreq_debug_disable_ratelimit(void){	unsigned long flags;	spin_lock_irqsave(&disable_ratelimit_lock, flags);	disable_ratelimit++;	spin_unlock_irqrestore(&disable_ratelimit_lock, flags);}void cpufreq_debug_printk(unsigned int type, const char *prefix,			const char *fmt, ...){	char s[256];	va_list args;	unsigned int len;	unsigned long flags;	WARN_ON(!prefix);	if (type & debug) {		spin_lock_irqsave(&disable_ratelimit_lock, flags);		if (!disable_ratelimit && debug_ratelimit					&& !printk_ratelimit()) {			spin_unlock_irqrestore(&disable_ratelimit_lock, flags);			return;		}		spin_unlock_irqrestore(&disable_ratelimit_lock, flags);		len = snprintf(s, 256, KERN_DEBUG "%s: ", prefix);		va_start(args, fmt);		len += vsnprintf(&s[len], (256 - len), fmt, args);		va_end(args);		printk(s);		WARN_ON(len < 5);	}}EXPORT_SYMBOL(cpufreq_debug_printk);module_param(debug, uint, 0644);MODULE_PARM_DESC(debug, "CPUfreq debugging: add 1 to debug core,"			" 2 to debug drivers, and 4 to debug governors.");module_param(debug_ratelimit, uint, 0644);MODULE_PARM_DESC(debug_ratelimit, "CPUfreq debugging:"					" set to 0 to disable ratelimiting.");#else static inline void cpufreq_debug_enable_ratelimit(void) { return; }static inline void cpufreq_debug_disable_ratelimit(void) { return; }#endif #ifndef CONFIG_SMPstatic unsigned long l_p_j_ref;static unsigned int  l_p_j_ref_freq;static void adjust_jiffies(unsigned long val, struct cpufreq_freqs *ci){	if (ci->flags & CPUFREQ_CONST_LOOPS)		return;	if (!l_p_j_ref_freq) {		l_p_j_ref = loops_per_jiffy;		l_p_j_ref_freq = ci->old;		dprintk("saving %lu as reference value for loops_per_jiffy; "			"freq is %u kHz\n", l_p_j_ref, l_p_j_ref_freq);	}	if ((val == CPUFREQ_PRECHANGE  && ci->old < ci->new) ||	    (val == CPUFREQ_POSTCHANGE && ci->old > ci->new) ||	    (val == CPUFREQ_RESUMECHANGE || val == CPUFREQ_SUSPENDCHANGE)) {		loops_per_jiffy = cpufreq_scale(l_p_j_ref, l_p_j_ref_freq,								ci->new);		dprintk("scaling loops_per_jiffy to %lu "			"for frequency %u kHz\n", loops_per_jiffy, ci->new);	}}#elsestatic inline void adjust_jiffies(unsigned long val, struct cpufreq_freqs *ci){	return;}#endifvoid cpufreq_notify_transition(struct cpufreq_freqs *freqs, unsigned int state){	struct cpufreq_policy *policy;	BUG_ON(irqs_disabled());	freqs->flags = cpufreq_driver->flags;	dprintk("notification %u of frequency transition to %u kHz\n",		state, freqs->new);	policy = per_cpu(cpufreq_cpu_data, freqs->cpu);	switch (state) {	case CPUFREQ_PRECHANGE:		if (!(cpufreq_driver->flags & CPUFREQ_CONST_LOOPS)) {			if ((policy) && (policy->cpu == freqs->cpu) &&			    (policy->cur) && (policy->cur != freqs->old)) {				dprintk("Warning: CPU frequency is"					" %u, cpufreq assumed %u kHz.\n",					freqs->old, policy->cur);				freqs->old = policy->cur;			}		}		srcu_notifier_call_chain(&cpufreq_transition_notifier_list,				CPUFREQ_PRECHANGE, freqs);		adjust_jiffies(CPUFREQ_PRECHANGE, freqs);		break;	case CPUFREQ_POSTCHANGE:		adjust_jiffies(CPUFREQ_POSTCHANGE, freqs);		srcu_notifier_call_chain(&cpufreq_transition_notifier_list,				CPUFREQ_POSTCHANGE, freqs);		if (likely(policy) && likely(policy->cpu == freqs->cpu))			policy->cur = freqs->new;		break;	}}EXPORT_SYMBOL_GPL(cpufreq_notify_transition);#ifdef CONFIG_HAS_EARLYSUSPENDstatic void cpufreq_early_suspend( struct early_suspend *h ){	cpufreq_rollback_policy();	pr_info("cpufreq_early_suspend!!!!\n");}static void cpufreq_early_resume( struct early_suspend *h ){	pr_info("cpufreq_early_resume!!!\n");}#endif static struct cpufreq_governor *__find_governor(const char *str_governor){	struct cpufreq_governor *t;	list_for_each_entry(t, &cpufreq_governor_list, governor_list)		if (!strnicmp(str_governor, t->name, CPUFREQ_NAME_LEN))			return t;	return NULL;}static int cpufreq_parse_governor(char *str_governor, unsigned int *policy,				struct cpufreq_governor **governor){	int err = -EINVAL;	if (!cpufreq_driver)		goto out;	if (cpufreq_driver->setpolicy) {		if (!strnicmp(str_governor, "performance", CPUFREQ_NAME_LEN)) {			*policy = CPUFREQ_POLICY_PERFORMANCE;			err = 0;		} else if (!strnicmp(str_governor, "powersave",						CPUFREQ_NAME_LEN)) {			*policy = CPUFREQ_POLICY_POWERSAVE;			err = 0;		}	} else if (cpufreq_driver->target) {		struct cpufreq_governor *t;		mutex_lock(&cpufreq_governor_mutex);		t = __find_governor(str_governor);		if (t == NULL) {			char *name = kasprintf(GFP_KERNEL, "cpufreq_%s",								str_governor);			if (name) {				int ret;				mutex_unlock(&cpufreq_governor_mutex);				ret = request_module("%s", name);				mutex_lock(&cpufreq_governor_mutex);				if (ret == 0)					t = __find_governor(str_governor);			}			kfree(name);		}		if (t != NULL) {			*governor = t;			err = 0;		}		mutex_unlock(&cpufreq_governor_mutex);	}out:	return err;}#define show_one(file_name, object)			\static ssize_t show_##file_name				\(struct cpufreq_policy *policy, char *buf)		\{							\	return sprintf(buf, "%u\n", policy->object);	\}show_one(cpuinfo_min_freq, cpuinfo.min_freq);show_one(cpuinfo_max_freq, cpuinfo.max_freq);show_one(cpuinfo_transition_latency, cpuinfo.transition_latency);show_one(scaling_min_freq, min);show_one(scaling_max_freq, max);show_one(scaling_cur_freq, cur);static int __cpufreq_set_policy(struct cpufreq_policy *data,				struct cpufreq_policy *policy);#define store_one(file_name, object)			\static ssize_t store_##file_name					\(struct cpufreq_policy *policy, const char *buf, size_t count)		\{									\	unsigned int ret = -EINVAL;					\	struct cpufreq_policy new_policy;				\									\	ret = cpufreq_get_policy(&new_policy, policy->cpu);		\	if (ret)							\		return -EINVAL;						\									\	ret = sscanf(buf, "%u", &new_policy.object);			\	if (ret != 1)							\		return -EINVAL;						\									\	ret = __cpufreq_set_policy(policy, &new_policy);		\	policy->user_policy.object = policy->object;			\									\	return ret ? ret : count;					\}store_one(scaling_min_freq, min);store_one(scaling_max_freq, max);static ssize_t store_scaling_boost_freq(struct cpufreq_policy *policy, 		const char *buf, size_t count){    unsigned int ret = 1;                     unsigned int retval = 0;                     struct cpufreq_policy new_policy;        	unsigned int delay = 0;                                         ret = cpufreq_get_policy(&new_policy, policy->cpu);     	retval = sscanf(buf, "%u", &delay);	if(!retval)		return -EINVAL; 	if(!delay)		return retval; 	if(delay == HIGHFREQ){		new_policy.min = policy->max; 		ret = __cpufreq_set_policy(policy, &new_policy);        		policy->user_policy.min = policy->min; 		return retval; 	} else if(delay == LOWFREQ) {		schedule_delayed_work(&cpufreq_tom_rollbackwq,0); 		return retval; 	} else {		printk(KERN_ERR"To configure boost HIGH[1] / ROLLBACK[2] \n"); 		return retval; 	}}static ssize_t show_scaling_boost_freq(struct cpufreq_policy *policy,		char *buf)		{								return sprintf(buf, "%u\n", policy->min);	}static ssize_t show_cpuinfo_cur_freq(struct cpufreq_policy *policy,					char *buf){	unsigned int cur_freq = __cpufreq_get(policy->cpu);	if (!cur_freq)		return sprintf(buf, "<unknown>");	return sprintf(buf, "%u\n", cur_freq);}static ssize_t show_scaling_governor(struct cpufreq_policy *policy, char *buf){	if (policy->policy == CPUFREQ_POLICY_POWERSAVE)		return sprintf(buf, "powersave\n");	else if (policy->policy == CPUFREQ_POLICY_PERFORMANCE)		return sprintf(buf, "performance\n");	else if (policy->governor)		return scnprintf(buf, CPUFREQ_NAME_LEN, "%s\n",				policy->governor->name);	return -EINVAL;}static ssize_t store_scaling_governor(struct cpufreq_policy *policy,					const char *buf, size_t count){	unsigned int ret = -EINVAL;	char	str_governor[16];	struct cpufreq_policy new_policy;	ret = cpufreq_get_policy(&new_policy, policy->cpu);	if (ret)		return ret;	ret = sscanf(buf, "%15s", str_governor);	if (ret != 1)		return -EINVAL;	if (cpufreq_parse_governor(str_governor, &new_policy.policy,						&new_policy.governor))		return -EINVAL;	ret = __cpufreq_set_policy(policy, &new_policy);	policy->user_policy.policy = policy->policy;	policy->user_policy.governor = policy->governor;	if (ret)		return ret;	else		return count;}static ssize_t show_scaling_driver(struct cpufreq_policy *policy, char *buf){	return scnprintf(buf, CPUFREQ_NAME_LEN, "%s\n", cpufreq_driver->name);}static ssize_t show_scaling_available_governors(struct cpufreq_policy *policy,						char *buf){	ssize_t i = 0;	struct cpufreq_governor *t;	if (!cpufreq_driver->target) {		i += sprintf(buf, "performance powersave");		goto out;	}	list_for_each_entry(t, &cpufreq_governor_list, governor_list) {		if (i >= (ssize_t) ((PAGE_SIZE / sizeof(char))		    - (CPUFREQ_NAME_LEN + 2)))			goto out;		i += scnprintf(&buf[i], CPUFREQ_NAME_LEN, "%s ", t->name);	}out:	i += sprintf(&buf[i], "\n");	return i;}static ssize_t show_cpus(const struct cpumask *mask, char *buf){	ssize_t i = 0;	unsigned int cpu;	for_each_cpu(cpu, mask) {		if (i)			i += scnprintf(&buf[i], (PAGE_SIZE - i - 2), " ");		i += scnprintf(&buf[i], (PAGE_SIZE - i - 2), "%u", cpu);		if (i >= (PAGE_SIZE - 5))			break;	}	i += sprintf(&buf[i], "\n");	return i;}static ssize_t show_related_cpus(struct cpufreq_policy *policy, char *buf){	if (cpumask_empty(policy->related_cpus))		return show_cpus(policy->cpus, buf);	return show_cpus(policy->related_cpus, buf);}static ssize_t show_affected_cpus(struct cpufreq_policy *policy, char *buf){	return show_cpus(policy->cpus, buf);}static ssize_t store_scaling_setspeed(struct cpufreq_policy *policy,					const char *buf, size_t count){	unsigned int freq = 0;	unsigned int ret;	if (!policy->governor || !policy->governor->store_setspeed)		return -EINVAL;	ret = sscanf(buf, "%u", &freq);	if (ret != 1)		return -EINVAL;	policy->governor->store_setspeed(policy, freq);	return count;}static ssize_t show_scaling_setspeed(struct cpufreq_policy *policy, char *buf){	if (!policy->governor || !policy->governor->show_setspeed)		return sprintf(buf, "<unsupported>\n");	return policy->governor->show_setspeed(policy, buf);}static ssize_t show_bios_limit(struct cpufreq_policy *policy, char *buf){	unsigned int limit;	int ret;	if (cpufreq_driver->bios_limit) {		ret = cpufreq_driver->bios_limit(policy->cpu, &limit);		if (!ret)			return sprintf(buf, "%u\n", limit);	}	return sprintf(buf, "%u\n", policy->cpuinfo.max_freq);}cpufreq_freq_attr_ro_perm(cpuinfo_cur_freq, 0400);cpufreq_freq_attr_ro(cpuinfo_min_freq);cpufreq_freq_attr_ro(cpuinfo_max_freq);cpufreq_freq_attr_ro(cpuinfo_transition_latency);cpufreq_freq_attr_ro(scaling_available_governors);cpufreq_freq_attr_ro(scaling_driver);cpufreq_freq_attr_ro(scaling_cur_freq);cpufreq_freq_attr_ro(bios_limit);cpufreq_freq_attr_ro(related_cpus);cpufreq_freq_attr_ro(affected_cpus);cpufreq_freq_attr_rw(scaling_min_freq);cpufreq_freq_attr_rw(scaling_max_freq);cpufreq_freq_attr_rw(scaling_governor);cpufreq_freq_attr_rw(scaling_setspeed);cpufreq_freq_attr_rw(scaling_boost_freq);static struct attribute *default_attrs[] = {	&cpuinfo_min_freq.attr,	&cpuinfo_max_freq.attr,	&cpuinfo_transition_latency.attr,	&scaling_min_freq.attr,	&scaling_max_freq.attr,	&affected_cpus.attr,	&related_cpus.attr,	&scaling_governor.attr,	&scaling_driver.attr,	&scaling_available_governors.attr,	&scaling_setspeed.attr,	&scaling_boost_freq.attr,			NULL};struct kobject *cpufreq_global_kobject;EXPORT_SYMBOL(cpufreq_global_kobject);#define to_policy(k) container_of(k, struct cpufreq_policy, kobj)#define to_attr(a) container_of(a, struct freq_attr, attr)static ssize_t show(struct kobject *kobj, struct attribute *attr, char *buf){	struct cpufreq_policy *policy = to_policy(kobj);	struct freq_attr *fattr = to_attr(attr);	ssize_t ret = -EINVAL;	policy = cpufreq_cpu_get(policy->cpu);	if (!policy)		goto no_policy;	if (lock_policy_rwsem_read(policy->cpu) < 0)		goto fail;	if (fattr->show)		ret = fattr->show(policy, buf);	else		ret = -EIO;	unlock_policy_rwsem_read(policy->cpu);fail:	cpufreq_cpu_put(policy);no_policy:	return ret;}static ssize_t store(struct kobject *kobj, struct attribute *attr,		     const char *buf, size_t count){	struct cpufreq_policy *policy = to_policy(kobj);	struct freq_attr *fattr = to_attr(attr);	ssize_t ret = -EINVAL;	policy = cpufreq_cpu_get(policy->cpu);	if (!policy)		goto no_policy;	if (lock_policy_rwsem_write(policy->cpu) < 0)		goto fail;	if (fattr->store)		ret = fattr->store(policy, buf, count);	else		ret = -EIO;	unlock_policy_rwsem_write(policy->cpu);fail:	cpufreq_cpu_put(policy);no_policy:	return ret;}static void cpufreq_sysfs_release(struct kobject *kobj){	struct cpufreq_policy *policy = to_policy(kobj);	dprintk("last reference is dropped\n");	complete(&policy->kobj_unregister);}static const struct sysfs_ops sysfs_ops = {	.show	= show,	.store	= store,};static struct kobj_type ktype_cpufreq = {	.sysfs_ops	= &sysfs_ops,	.default_attrs	= default_attrs,	.release	= cpufreq_sysfs_release,};static int cpufreq_add_dev_policy(unsigned int cpu,				  struct cpufreq_policy *policy,				  struct sys_device *sys_dev){	int ret = 0;#ifdef CONFIG_SMP	unsigned long flags;	unsigned int j;#ifdef CONFIG_HOTPLUG_CPU	struct cpufreq_governor *gov;	gov = __find_governor(per_cpu(cpufreq_cpu_governor, cpu));	if (gov) {		policy->governor = gov;		dprintk("Restoring governor %s for cpu %d\n",		       policy->governor->name, cpu);	}#endif	for_each_cpu(j, policy->cpus) {		struct cpufreq_policy *managed_policy;		if (cpu == j)			continue;		managed_policy = cpufreq_cpu_get(j);		if (unlikely(managed_policy)) {			unlock_policy_rwsem_write(cpu);			per_cpu(cpufreq_policy_cpu, cpu) = managed_policy->cpu;			if (lock_policy_rwsem_write(cpu) < 0) {								if (cpufreq_driver->exit)					cpufreq_driver->exit(policy);				cpufreq_cpu_put(managed_policy);				return -EBUSY;			}			spin_lock_irqsave(&cpufreq_driver_lock, flags);			cpumask_copy(managed_policy->cpus, policy->cpus);			per_cpu(cpufreq_cpu_data, cpu) = managed_policy;			spin_unlock_irqrestore(&cpufreq_driver_lock, flags);			dprintk("CPU already managed, adding link\n");			ret = sysfs_create_link(&sys_dev->kobj,						&managed_policy->kobj,						"cpufreq");			if (ret)				cpufreq_cpu_put(managed_policy);			if (cpufreq_driver->exit)				cpufreq_driver->exit(policy);			if (!ret)				return 1;			else				return ret;		}	}#endif	return ret;}static int cpufreq_add_dev_symlink(unsigned int cpu,				   struct cpufreq_policy *policy){	unsigned int j;	int ret = 0;	for_each_cpu(j, policy->cpus) {		struct cpufreq_policy *managed_policy;		struct sys_device *cpu_sys_dev;		if (j == cpu)			continue;		if (!cpu_online(j))			continue;		dprintk("CPU %u already managed, adding link\n", j);		managed_policy = cpufreq_cpu_get(cpu);		cpu_sys_dev = get_cpu_sysdev(j);		ret = sysfs_create_link(&cpu_sys_dev->kobj, &policy->kobj,					"cpufreq");		if (ret) {			cpufreq_cpu_put(managed_policy);			return ret;		}	}	return ret;}static int cpufreq_add_dev_interface(unsigned int cpu,				     struct cpufreq_policy *policy,				     struct sys_device *sys_dev){	struct cpufreq_policy new_policy;	struct freq_attr **drv_attr;	unsigned long flags;	int ret = 0;	unsigned int j;	ret = kobject_init_and_add(&policy->kobj, &ktype_cpufreq,				   &sys_dev->kobj, "cpufreq");	if (ret)		return ret;	drv_attr = cpufreq_driver->attr;	while ((drv_attr) && (*drv_attr)) {		ret = sysfs_create_file(&policy->kobj, &((*drv_attr)->attr));		if (ret)			goto err_out_kobj_put;		drv_attr++;	}	if (cpufreq_driver->get) {		ret = sysfs_create_file(&policy->kobj, &cpuinfo_cur_freq.attr);		if (ret)			goto err_out_kobj_put;	}	if (cpufreq_driver->target) {		ret = sysfs_create_file(&policy->kobj, &scaling_cur_freq.attr);		if (ret)			goto err_out_kobj_put;	}	if (cpufreq_driver->bios_limit) {		ret = sysfs_create_file(&policy->kobj, &bios_limit.attr);		if (ret)			goto err_out_kobj_put;	}	spin_lock_irqsave(&cpufreq_driver_lock, flags);	for_each_cpu(j, policy->cpus) {	if (!cpu_online(j))		continue;		per_cpu(cpufreq_cpu_data, j) = policy;		per_cpu(cpufreq_policy_cpu, j) = policy->cpu;	}	spin_unlock_irqrestore(&cpufreq_driver_lock, flags);	ret = cpufreq_add_dev_symlink(cpu, policy);	if (ret)		goto err_out_kobj_put;	memcpy(&new_policy, policy, sizeof(struct cpufreq_policy));		policy->governor = NULL;	ret = __cpufreq_set_policy(policy, &new_policy);	policy->user_policy.policy = policy->policy;	policy->user_policy.governor = policy->governor;	if (ret) {		dprintk("setting policy failed\n");		if (cpufreq_driver->exit)			cpufreq_driver->exit(policy);	}	return ret;err_out_kobj_put:	kobject_put(&policy->kobj);	wait_for_completion(&policy->kobj_unregister);	return ret;}static int cpufreq_add_dev(struct sys_device *sys_dev){	unsigned int cpu = sys_dev->id;	int ret = 0, found = 0;	struct cpufreq_policy *policy;	unsigned long flags;	unsigned int j;#ifdef CONFIG_HOTPLUG_CPU	int sibling;#endif	if (cpu_is_offline(cpu))		return 0;	cpufreq_debug_disable_ratelimit();	dprintk("adding CPU %u\n", cpu);#ifdef CONFIG_SMP	policy = cpufreq_cpu_get(cpu);	if (unlikely(policy)) {		cpufreq_cpu_put(policy);		cpufreq_debug_enable_ratelimit();		return 0;	}#endif	if (!try_module_get(cpufreq_driver->owner)) {		ret = -EINVAL;		goto module_out;	}	ret = -ENOMEM;	policy = kzalloc(sizeof(struct cpufreq_policy), GFP_KERNEL);	if (!policy)		goto nomem_out;	if (!alloc_cpumask_var(&policy->cpus, GFP_KERNEL))		goto err_free_policy;	if (!zalloc_cpumask_var(&policy->related_cpus, GFP_KERNEL))		goto err_free_cpumask;	policy->cpu = cpu;	cpumask_copy(policy->cpus, cpumask_of(cpu));	per_cpu(cpufreq_policy_cpu, cpu) = cpu;	ret = (lock_policy_rwsem_write(cpu) < 0);	WARN_ON(ret);	init_completion(&policy->kobj_unregister);	INIT_WORK(&policy->update, handle_update);#ifdef CONFIG_HOTPLUG_CPU	for_each_online_cpu(sibling) {		struct cpufreq_policy *cp = per_cpu(cpufreq_cpu_data, sibling);		if (cp && cp->governor &&		    (cpumask_test_cpu(cpu, cp->related_cpus))) {			policy->governor = cp->governor;			found = 1;			break;		}	}#endif	if (!found)		policy->governor = CPUFREQ_DEFAULT_GOVERNOR;	ret = cpufreq_driver->init(policy);	if (ret) {		dprintk("initialization failed\n");		goto err_unlock_policy;	}	policy->user_policy.min = policy->min;	policy->user_policy.max = policy->max;	blocking_notifier_call_chain(&cpufreq_policy_notifier_list,				     CPUFREQ_START, policy);	ret = cpufreq_add_dev_policy(cpu, policy, sys_dev);	if (ret) {		if (ret > 0)			ret = 0;		goto err_unlock_policy;	}	ret = cpufreq_add_dev_interface(cpu, policy, sys_dev);	if (ret)		goto err_out_unregister;	unlock_policy_rwsem_write(cpu);	kobject_uevent(&policy->kobj, KOBJ_ADD);	module_put(cpufreq_driver->owner);	dprintk("initialization complete\n");	cpufreq_debug_enable_ratelimit();	return 0;err_out_unregister:	spin_lock_irqsave(&cpufreq_driver_lock, flags);	for_each_cpu(j, policy->cpus)		per_cpu(cpufreq_cpu_data, j) = NULL;	spin_unlock_irqrestore(&cpufreq_driver_lock, flags);	kobject_put(&policy->kobj);	wait_for_completion(&policy->kobj_unregister);err_unlock_policy:	unlock_policy_rwsem_write(cpu);	free_cpumask_var(policy->related_cpus);err_free_cpumask:	free_cpumask_var(policy->cpus);err_free_policy:	kfree(policy);nomem_out:	module_put(cpufreq_driver->owner);module_out:	cpufreq_debug_enable_ratelimit();	return ret;}static int __cpufreq_remove_dev(struct sys_device *sys_dev){	unsigned int cpu = sys_dev->id;	unsigned long flags;	struct cpufreq_policy *data;	struct kobject *kobj;	struct completion *cmp;#ifdef CONFIG_SMP	struct sys_device *cpu_sys_dev;	unsigned int j;#endif	cpufreq_debug_disable_ratelimit();	dprintk("unregistering CPU %u\n", cpu);	spin_lock_irqsave(&cpufreq_driver_lock, flags);	data = per_cpu(cpufreq_cpu_data, cpu);	if (!data) {		spin_unlock_irqrestore(&cpufreq_driver_lock, flags);		cpufreq_debug_enable_ratelimit();		unlock_policy_rwsem_write(cpu);		return -EINVAL;	}	per_cpu(cpufreq_cpu_data, cpu) = NULL;#ifdef CONFIG_SMP	if (unlikely(cpu != data->cpu)) {		dprintk("removing link\n");		cpumask_clear_cpu(cpu, data->cpus);		spin_unlock_irqrestore(&cpufreq_driver_lock, flags);		kobj = &sys_dev->kobj;		cpufreq_cpu_put(data);		cpufreq_debug_enable_ratelimit();		unlock_policy_rwsem_write(cpu);		sysfs_remove_link(kobj, "cpufreq");		return 0;	}#endif#ifdef CONFIG_SMP#ifdef CONFIG_HOTPLUG_CPU	strncpy(per_cpu(cpufreq_cpu_governor, cpu), data->governor->name,			CPUFREQ_NAME_LEN);#endif	if (unlikely(cpumask_weight(data->cpus) > 1)) {		for_each_cpu(j, data->cpus) {			if (j == cpu)				continue;			per_cpu(cpufreq_cpu_data, j) = NULL;		}	}	spin_unlock_irqrestore(&cpufreq_driver_lock, flags);	if (unlikely(cpumask_weight(data->cpus) > 1)) {		for_each_cpu(j, data->cpus) {			if (j == cpu)				continue;			dprintk("removing link for cpu %u\n", j);#ifdef CONFIG_HOTPLUG_CPU			strncpy(per_cpu(cpufreq_cpu_governor, j),				data->governor->name, CPUFREQ_NAME_LEN);#endif			cpu_sys_dev = get_cpu_sysdev(j);			kobj = &cpu_sys_dev->kobj;			unlock_policy_rwsem_write(cpu);			sysfs_remove_link(kobj, "cpufreq");			lock_policy_rwsem_write(cpu);			cpufreq_cpu_put(data);		}	}#else	spin_unlock_irqrestore(&cpufreq_driver_lock, flags);#endif	if (cpufreq_driver->target)		__cpufreq_governor(data, CPUFREQ_GOV_STOP);	kobj = &data->kobj;	cmp = &data->kobj_unregister;	unlock_policy_rwsem_write(cpu);	kobject_put(kobj);	dprintk("waiting for dropping of refcount\n");	wait_for_completion(cmp);	dprintk("wait complete\n");	lock_policy_rwsem_write(cpu);	if (cpufreq_driver->exit)		cpufreq_driver->exit(data);	unlock_policy_rwsem_write(cpu);	free_cpumask_var(data->related_cpus);	free_cpumask_var(data->cpus);	kfree(data);	per_cpu(cpufreq_cpu_data, cpu) = NULL;	cpufreq_debug_enable_ratelimit();	return 0;}static int cpufreq_remove_dev(struct sys_device *sys_dev){	unsigned int cpu = sys_dev->id;	int retval;	if (cpu_is_offline(cpu))		return 0;	if (unlikely(lock_policy_rwsem_write(cpu)))		BUG();	retval = __cpufreq_remove_dev(sys_dev);	return retval;}static work_func_t handle_rollback(void *data){	dprintk("handle_rollback for cpu called\n");	cpufreq_rollback_policy();	return 0; }static void handle_update(struct work_struct *work){	struct cpufreq_policy *policy =		container_of(work, struct cpufreq_policy, update);	unsigned int cpu = policy->cpu;	dprintk("handle_update for cpu %u called\n", cpu);	cpufreq_update_policy(cpu);}static void cpufreq_out_of_sync(unsigned int cpu, unsigned int old_freq,				unsigned int new_freq){	struct cpufreq_freqs freqs;	dprintk("Warning: CPU frequency out of sync: cpufreq and timing "	       "core thinks of %u, is %u kHz.\n", old_freq, new_freq);	freqs.cpu = cpu;	freqs.old = old_freq;	freqs.new = new_freq;	cpufreq_notify_transition(&freqs, CPUFREQ_PRECHANGE);	cpufreq_notify_transition(&freqs, CPUFREQ_POSTCHANGE);}unsigned int cpufreq_quick_get(unsigned int cpu){	struct cpufreq_policy *policy = cpufreq_cpu_get(cpu);	unsigned int ret_freq = 0;	if (policy) {		ret_freq = policy->cur;		cpufreq_cpu_put(policy);	}	return ret_freq;}EXPORT_SYMBOL(cpufreq_quick_get);static unsigned int __cpufreq_get(unsigned int cpu){	struct cpufreq_policy *policy = per_cpu(cpufreq_cpu_data, cpu);	unsigned int ret_freq = 0;	if (!cpufreq_driver->get)		return ret_freq;	ret_freq = cpufreq_driver->get(cpu);	if (ret_freq && policy->cur &&		!(cpufreq_driver->flags & CPUFREQ_CONST_LOOPS)) {		if (unlikely(ret_freq != policy->cur)) {			cpufreq_out_of_sync(cpu, policy->cur, ret_freq);			schedule_work(&policy->update);		}	}	return ret_freq;}unsigned int cpufreq_get(unsigned int cpu){	unsigned int ret_freq = 0;	struct cpufreq_policy *policy = cpufreq_cpu_get(cpu);	if (!policy)		goto out;	if (unlikely(lock_policy_rwsem_read(cpu)))		goto out_policy;	ret_freq = __cpufreq_get(cpu);	unlock_policy_rwsem_read(cpu);out_policy:	cpufreq_cpu_put(policy);out:	return ret_freq;}EXPORT_SYMBOL(cpufreq_get);static int cpufreq_suspend(struct sys_device *sysdev, pm_message_t pmsg){	int ret = 0;	int cpu = sysdev->id;	struct cpufreq_policy *cpu_policy;	dprintk("suspending cpu %u\n", cpu);	if (!cpu_online(cpu))		return 0;	cpu_policy = cpufreq_cpu_get(cpu);	if (!cpu_policy)		return -EINVAL;	if (unlikely(cpu_policy->cpu != cpu))		goto out;	if (cpufreq_driver->suspend) {		ret = cpufreq_driver->suspend(cpu_policy, pmsg);		if (ret)			printk(KERN_ERR "cpufreq: suspend failed in ->suspend "					"step on CPU %u\n", cpu_policy->cpu);	}out:	cpufreq_cpu_put(cpu_policy);	return ret;}static int cpufreq_resume(struct sys_device *sysdev){	int ret = 0;	int cpu = sysdev->id;	struct cpufreq_policy *cpu_policy;	dprintk("resuming cpu %u\n", cpu);	if (!cpu_online(cpu))		return 0;	cpu_policy = cpufreq_cpu_get(cpu);	if (!cpu_policy)		return -EINVAL;	if (unlikely(cpu_policy->cpu != cpu))		goto fail;	if (cpufreq_driver->resume) {		ret = cpufreq_driver->resume(cpu_policy);		if (ret) {			printk(KERN_ERR "cpufreq: resume failed in ->resume "					"step on CPU %u\n", cpu_policy->cpu);			goto fail;		}	}	schedule_work(&cpu_policy->update);fail:	cpufreq_cpu_put(cpu_policy);	return ret;}static struct sysdev_driver cpufreq_sysdev_driver = {	.add		= cpufreq_add_dev,	.remove		= cpufreq_remove_dev,	.suspend	= cpufreq_suspend,	.resume		= cpufreq_resume,};int cpufreq_register_notifier(struct notifier_block *nb, unsigned int list){	int ret;	WARN_ON(!init_cpufreq_transition_notifier_list_called);	switch (list) {	case CPUFREQ_TRANSITION_NOTIFIER:		ret = srcu_notifier_chain_register(				&cpufreq_transition_notifier_list, nb);		break;	case CPUFREQ_POLICY_NOTIFIER:		ret = blocking_notifier_chain_register(				&cpufreq_policy_notifier_list, nb);		break;	default:		ret = -EINVAL;	}	return ret;}EXPORT_SYMBOL(cpufreq_register_notifier);int cpufreq_unregister_notifier(struct notifier_block *nb, unsigned int list){	int ret;	switch (list) {	case CPUFREQ_TRANSITION_NOTIFIER:		ret = srcu_notifier_chain_unregister(				&cpufreq_transition_notifier_list, nb);		break;	case CPUFREQ_POLICY_NOTIFIER:		ret = blocking_notifier_chain_unregister(				&cpufreq_policy_notifier_list, nb);		break;	default:		ret = -EINVAL;	}	return ret;}EXPORT_SYMBOL(cpufreq_unregister_notifier);int __cpufreq_driver_target(struct cpufreq_policy *policy,			    unsigned int target_freq,			    unsigned int relation){	int retval = -EINVAL;	dprintk("target for CPU %u: %u kHz, relation %u\n", policy->cpu,		target_freq, relation);	if (cpu_online(policy->cpu) && cpufreq_driver->target)		retval = cpufreq_driver->target(policy, target_freq, relation);	return retval;}EXPORT_SYMBOL_GPL(__cpufreq_driver_target);int cpufreq_driver_target_max_bycpuid(unsigned int cpu,			  unsigned int is_max){	int ret = -EINVAL;	unsigned int target_freq; 	unsigned int relation; 	struct cpufreq_policy *policy = cpufreq_cpu_get(cpu);	if (!policy)		goto no_policy;	if(is_max){		target_freq = policy->max;		relation = CPUFREQ_RELATION_H; 	}	else{		target_freq = policy->min;		relation = CPUFREQ_RELATION_L;	}		if (unlikely(lock_policy_rwsem_write(policy->cpu)))		goto fail;	ret = __cpufreq_driver_target(policy, target_freq, relation);	unlock_policy_rwsem_write(policy->cpu);	dprintk(KERN_ERR" Fourth cpu [%u] target_freq [%u] relation [%u] \n", 			cpu,			target_freq,			relation); fail:	cpufreq_cpu_put(policy);no_policy:	return ret;}EXPORT_SYMBOL_GPL(cpufreq_driver_target_max_bycpuid);int cpufreq_driver_target(struct cpufreq_policy *policy,			  unsigned int target_freq,			  unsigned int relation){	int ret = -EINVAL;	policy = cpufreq_cpu_get(policy->cpu);	if (!policy)		goto no_policy;	if (unlikely(lock_policy_rwsem_write(policy->cpu)))		goto fail;	ret = __cpufreq_driver_target(policy, target_freq, relation);	unlock_policy_rwsem_write(policy->cpu);fail:	cpufreq_cpu_put(policy);no_policy:	return ret;}EXPORT_SYMBOL_GPL(cpufreq_driver_target);int __cpufreq_driver_getavg(struct cpufreq_policy *policy, unsigned int cpu){	int ret = 0;	policy = cpufreq_cpu_get(policy->cpu);	if (!policy)		return -EINVAL;	if (cpu_online(cpu) && cpufreq_driver->getavg)		ret = cpufreq_driver->getavg(policy, cpu);	cpufreq_cpu_put(policy);	return ret;}EXPORT_SYMBOL_GPL(__cpufreq_driver_getavg);static int __cpufreq_governor(struct cpufreq_policy *policy,					unsigned int event){	int ret;#ifdef CONFIG_CPU_FREQ_GOV_PERFORMANCE	struct cpufreq_governor *gov = &cpufreq_gov_performance;#else	struct cpufreq_governor *gov = NULL;#endif	if (policy->governor->max_transition_latency &&	    policy->cpuinfo.transition_latency >	    policy->governor->max_transition_latency) {		if (!gov)			return -EINVAL;		else {			printk(KERN_WARNING "%s governor failed, too long"			       " transition latency of HW, fallback"			       " to %s governor\n",			       policy->governor->name,			       gov->name);			policy->governor = gov;		}	}	if (!try_module_get(policy->governor->owner))		return -EINVAL;	dprintk("__cpufreq_governor for CPU %u, event %u\n",						policy->cpu, event);	ret = policy->governor->governor(policy, event);	if ((event != CPUFREQ_GOV_START) || ret)		module_put(policy->governor->owner);	if ((event == CPUFREQ_GOV_STOP) && !ret)		module_put(policy->governor->owner);	return ret;}int cpufreq_register_governor(struct cpufreq_governor *governor){	int err;	if (!governor)		return -EINVAL;	mutex_lock(&cpufreq_governor_mutex);	err = -EBUSY;	if (__find_governor(governor->name) == NULL) {		err = 0;		list_add(&governor->governor_list, &cpufreq_governor_list);	}	mutex_unlock(&cpufreq_governor_mutex);	return err;}EXPORT_SYMBOL_GPL(cpufreq_register_governor);void cpufreq_unregister_governor(struct cpufreq_governor *governor){#ifdef CONFIG_HOTPLUG_CPU	int cpu;#endif	if (!governor)		return;#ifdef CONFIG_HOTPLUG_CPU	for_each_present_cpu(cpu) {		if (cpu_online(cpu))			continue;		if (!strcmp(per_cpu(cpufreq_cpu_governor, cpu), governor->name))			strcpy(per_cpu(cpufreq_cpu_governor, cpu), "\0");	}#endif	mutex_lock(&cpufreq_governor_mutex);	list_del(&governor->governor_list);	mutex_unlock(&cpufreq_governor_mutex);	return;}EXPORT_SYMBOL_GPL(cpufreq_unregister_governor);int cpufreq_get_policy(struct cpufreq_policy *policy, unsigned int cpu){	struct cpufreq_policy *cpu_policy;	if (!policy)		return -EINVAL;	cpu_policy = cpufreq_cpu_get(cpu);	if (!cpu_policy)		return -EINVAL;	memcpy(policy, cpu_policy, sizeof(struct cpufreq_policy));	cpufreq_cpu_put(cpu_policy);	return 0;}EXPORT_SYMBOL(cpufreq_get_policy);static int __cpufreq_set_policy(struct cpufreq_policy *data,				struct cpufreq_policy *policy){	int ret = 0;	cpufreq_debug_disable_ratelimit();	dprintk("setting new policy for CPU %u: %u - %u kHz\n", policy->cpu,		policy->min, policy->max);	memcpy(&policy->cpuinfo, &data->cpuinfo,				sizeof(struct cpufreq_cpuinfo));	if (policy->min > data->max || policy->max < data->min) {		ret = -EINVAL;		goto error_out;	}	ret = cpufreq_driver->verify(policy);	if (ret)		goto error_out;	blocking_notifier_call_chain(&cpufreq_policy_notifier_list,			CPUFREQ_ADJUST, policy);	blocking_notifier_call_chain(&cpufreq_policy_notifier_list,			CPUFREQ_INCOMPATIBLE, policy);	ret = cpufreq_driver->verify(policy);	if (ret)		goto error_out;	blocking_notifier_call_chain(&cpufreq_policy_notifier_list,			CPUFREQ_NOTIFY, policy);	data->min = policy->min;	data->max = policy->max;	dprintk("new min and max freqs are %u - %u kHz\n",					data->min, data->max);	if (cpufreq_driver->setpolicy) {		data->policy = policy->policy;		dprintk("setting range\n");		ret = cpufreq_driver->setpolicy(policy);	} else {		if (policy->governor != data->governor) {						struct cpufreq_governor *old_gov = data->governor;			dprintk("governor switch\n");			if (data->governor)				__cpufreq_governor(data, CPUFREQ_GOV_STOP);			data->governor = policy->governor;			if (__cpufreq_governor(data, CPUFREQ_GOV_START)) {								dprintk("starting governor %s failed\n",							data->governor->name);				if (old_gov) {					data->governor = old_gov;					__cpufreq_governor(data,							   CPUFREQ_GOV_START);				}				ret = -EINVAL;				goto error_out;			}					}		dprintk("governor: change or update limits\n");		__cpufreq_governor(data, CPUFREQ_GOV_LIMITS);	}error_out:	cpufreq_debug_enable_ratelimit();	return ret;}int cpufreq_update_policy(unsigned int cpu){	struct cpufreq_policy *data = cpufreq_cpu_get(cpu);	struct cpufreq_policy policy;	int ret;	if (!data) {		ret = -ENODEV;		goto no_policy;	}	if (unlikely(lock_policy_rwsem_write(cpu))) {		ret = -EINVAL;		goto fail;	}	dprintk("updating policy for CPU %u\n", cpu);	memcpy(&policy, data, sizeof(struct cpufreq_policy));	policy.min = data->user_policy.min;	policy.max = data->user_policy.max;	policy.policy = data->user_policy.policy;	policy.governor = data->user_policy.governor;	if (cpufreq_driver->get) {		policy.cur = cpufreq_driver->get(cpu);		if (!data->cur) {			dprintk("Driver did not initialize current freq");			data->cur = policy.cur;		} else {			if (data->cur != policy.cur)				cpufreq_out_of_sync(cpu, data->cur,								policy.cur);		}	}	ret = __cpufreq_set_policy(data, &policy);	unlock_policy_rwsem_write(cpu);fail:	cpufreq_cpu_put(data);no_policy:	return ret;}EXPORT_SYMBOL(cpufreq_update_policy);int cpufreq_rollback_policy(void){	unsigned int cpu = 0;	unsigned int min_freq = 0; 	struct cpufreq_policy *data = cpufreq_cpu_get(cpu);	struct cpufreq_policy policy;	int ret;	struct cpufreq_frequency_table *table;	table = cpufreq_frequency_get_table(cpu);	min_freq = table[0].frequency; 	if (!data) {		ret = -ENODEV;		goto no_policy;	}	if (unlikely(lock_policy_rwsem_write(cpu))) {		ret = -EINVAL;		goto fail;	}	dprintk("updating policy for CPU %u\n", cpu);	memcpy(&policy, data, sizeof(struct cpufreq_policy));	policy.min = min_freq;	ret = __cpufreq_set_policy(data, &policy);	policy.user_policy.min = policy.min;	unlock_policy_rwsem_write(cpu);fail:	cpufreq_cpu_put(data);no_policy:	return ret;}EXPORT_SYMBOL(cpufreq_rollback_policy);static int __cpuinit cpufreq_cpu_callback(struct notifier_block *nfb,					unsigned long action, void *hcpu){	unsigned int cpu = (unsigned long)hcpu;	struct sys_device *sys_dev;	sys_dev = get_cpu_sysdev(cpu);	if (sys_dev) {		switch (action) {		case CPU_ONLINE:		case CPU_ONLINE_FROZEN:			cpufreq_add_dev(sys_dev);			break;		case CPU_DOWN_PREPARE:		case CPU_DOWN_PREPARE_FROZEN:			if (unlikely(lock_policy_rwsem_write(cpu)))				BUG();			__cpufreq_remove_dev(sys_dev);			break;		case CPU_DOWN_FAILED:		case CPU_DOWN_FAILED_FROZEN:			cpufreq_add_dev(sys_dev);			break;		}	}	return NOTIFY_OK;}static struct notifier_block __refdata cpufreq_cpu_notifier ={    .notifier_call = cpufreq_cpu_callback,};int cpufreq_register_driver(struct cpufreq_driver *driver_data){	unsigned long flags;	int ret;	if (!driver_data || !driver_data->verify || !driver_data->init ||	    ((!driver_data->setpolicy) && (!driver_data->target)))		return -EINVAL;	dprintk("trying to register driver %s\n", driver_data->name);	if (driver_data->setpolicy)		driver_data->flags |= CPUFREQ_CONST_LOOPS;	spin_lock_irqsave(&cpufreq_driver_lock, flags);	if (cpufreq_driver) {		spin_unlock_irqrestore(&cpufreq_driver_lock, flags);		return -EBUSY;	}	cpufreq_driver = driver_data;	spin_unlock_irqrestore(&cpufreq_driver_lock, flags);	ret = sysdev_driver_register(&cpu_sysdev_class,					&cpufreq_sysdev_driver);	if ((!ret) && !(cpufreq_driver->flags & CPUFREQ_STICKY)) {		int i;		ret = -ENODEV;		for (i = 0; i < nr_cpu_ids; i++)			if (cpu_possible(i) && per_cpu(cpufreq_cpu_data, i)) {				ret = 0;				break;			}		if (ret) {			dprintk("no CPU initialized for driver %s\n",							driver_data->name);			sysdev_driver_unregister(&cpu_sysdev_class,						&cpufreq_sysdev_driver);			spin_lock_irqsave(&cpufreq_driver_lock, flags);			cpufreq_driver = NULL;			spin_unlock_irqrestore(&cpufreq_driver_lock, flags);		}	}	if (!ret) {		register_hotcpu_notifier(&cpufreq_cpu_notifier);		dprintk("driver %s up and running\n", driver_data->name);		cpufreq_debug_enable_ratelimit();	}#ifdef CONFIG_HAS_EARLYSUSPEND	if(cpufreq_driver != NULL) {		cpufreq_driver->cpufreq_early_suspend.level = EARLY_SUSPEND_LEVEL_BLANK_SCREEN;		cpufreq_driver->cpufreq_early_suspend.suspend = cpufreq_early_suspend;		cpufreq_driver->cpufreq_early_suspend.resume = cpufreq_early_resume;		register_early_suspend(&cpufreq_driver->cpufreq_early_suspend);	}#endif	return ret;}EXPORT_SYMBOL_GPL(cpufreq_register_driver);int cpufreq_unregister_driver(struct cpufreq_driver *driver){	unsigned long flags;	cpufreq_debug_disable_ratelimit();	if (!cpufreq_driver || (driver != cpufreq_driver)) {		cpufreq_debug_enable_ratelimit();		return -EINVAL;	}	dprintk("unregistering driver %s\n", driver->name);	sysdev_driver_unregister(&cpu_sysdev_class, &cpufreq_sysdev_driver);	unregister_hotcpu_notifier(&cpufreq_cpu_notifier);	spin_lock_irqsave(&cpufreq_driver_lock, flags);	cpufreq_driver = NULL;	spin_unlock_irqrestore(&cpufreq_driver_lock, flags);	return 0;}EXPORT_SYMBOL_GPL(cpufreq_unregister_driver);static int __init cpufreq_core_init(void){	int cpu;	for_each_possible_cpu(cpu) {		per_cpu(cpufreq_policy_cpu, cpu) = -1;		init_rwsem(&per_cpu(cpu_policy_rwsem, cpu));	}	cpufreq_global_kobject = kobject_create_and_add("cpufreq",						&cpu_sysdev_class.kset.kobj);	BUG_ON(!cpufreq_global_kobject);	return 0;}core_initcall(cpufreq_core_init);